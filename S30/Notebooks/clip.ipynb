{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7034a003-b6bb-4d9d-9fa2-f0dee64df980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 4929/4929 [00:54<00:00, 90.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Avg Loss: 0.0209\n",
      "GPU Memory: 2.97 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 4929/4929 [00:52<00:00, 94.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Avg Loss: -0.0000\n",
      "GPU Memory: 2.97 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 4929/4929 [00:54<00:00, 91.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Avg Loss: -0.0002\n",
      "GPU Memory: 2.97 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 4929/4929 [00:52<00:00, 93.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Avg Loss: -0.0002\n",
      "GPU Memory: 2.97 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 4929/4929 [00:50<00:00, 96.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Avg Loss: -0.0002\n",
      "GPU Memory: 2.97 GB\n",
      "Projection layer training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CLIPEmbeddingDataset(Dataset):\n",
    "    def __init__(self, h5_file, tokenizer, max_length=512):\n",
    "        self.h5_file = h5_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        with h5py.File(self.h5_file, 'r') as hf:\n",
    "            self.length = len(hf['image_embeddings'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, 'r') as hf:\n",
    "            clip_embedding = hf['image_embeddings'][idx]\n",
    "            text = hf['text'][idx]\n",
    "            \n",
    "            clip_embedding = torch.tensor(clip_embedding, dtype=torch.float32)\n",
    "            \n",
    "            if isinstance(text, bytes):\n",
    "                text = text.decode('utf-8')\n",
    "\n",
    "        encoded_text = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'clip_embedding': clip_embedding,\n",
    "            'input_ids': encoded_text['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded_text['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, clip_dim=512, phi_dim=2048):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(clip_dim, phi_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    h5_file_path = 'clip_embeddings_150k.h5'\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    phi_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=torch.float16).to(device)\n",
    "    phi_embed = phi_model.model.embed_tokens\n",
    "\n",
    "    dataset = CLIPEmbeddingDataset(h5_file_path, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "    projection_layer = ProjectionLayer().to(device)\n",
    "    optimizer = torch.optim.AdamW(projection_layer.parameters(), lr=1e-4)\n",
    "    cos_sim = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            clip_embeddings = batch['clip_embedding'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                phi_embeddings = phi_embed(input_ids)[:, 0, :]\n",
    "            \n",
    "            projected_embeddings = projection_layer(clip_embeddings)\n",
    "            \n",
    "            loss = 1 - cos_sim(projected_embeddings, phi_embeddings).mean()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {total_loss/len(dataloader):.4f}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "    torch.save(projection_layer.state_dict(), 'trained_projection_layer.pth')\n",
    "    print(\"Projection layer training complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5a3bc3d-606d-496c-b36d-3f288819f17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection layer loaded successfully\n",
      "Input shape: torch.Size([1, 512])\n",
      "Output shape: torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the ProjectionLayer class\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, clip_dim=512, phi_dim=2048):  # Adjust phi_dim if necessary\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(clip_dim, phi_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "# Create an instance of the ProjectionLayer\n",
    "projection_layer = ProjectionLayer()\n",
    "\n",
    "# Load the state dict\n",
    "state_dict = torch.load('trained_projection_layer.pth', weights_only=True)\n",
    "projection_layer.load_state_dict(state_dict)\n",
    "\n",
    "# Test it with a dummy input\n",
    "dummy_clip_embedding = torch.randn(1, 512)  # Assuming CLIP embeddings are 512-dimensional\n",
    "projected = projection_layer(dummy_clip_embedding)\n",
    "\n",
    "print(\"Projection layer loaded successfully\")\n",
    "print(f\"Input shape: {dummy_clip_embedding.shape}\")\n",
    "print(f\"Output shape: {projected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f173c45-19f6-4e89-afbc-a16c0087868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
    "\n",
    "print(\"Phi model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c26c45a6-c3b1-4a05-8817-b7e837992fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined model created successfully\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleMultiModalPhi(nn.Module):\n",
    "    def __init__(self, phi_model, projection_layer):\n",
    "        super().__init__()\n",
    "        self.phi = phi_model\n",
    "        self.projection = projection_layer\n",
    "    \n",
    "    def forward(self, text_input, image_embedding=None):\n",
    "        if image_embedding is not None:\n",
    "            projected_image = self.projection(image_embedding)\n",
    "            text_embeds = self.phi.get_input_embeddings()(text_input)\n",
    "            combined_input = torch.cat([projected_image.unsqueeze(1), text_embeds], dim=1)\n",
    "            output = self.phi(inputs_embeds=combined_input)\n",
    "        else:\n",
    "            output = self.phi(text_input)\n",
    "        return output\n",
    "\n",
    "combined_model = SimpleMultiModalPhi(phi_model, projection_layer)\n",
    "print(\"Combined model created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39630d81-4f31-43bc-b823-7bd95b2f5be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-only output shape: torch.Size([1, 4, 51200])\n",
      "Text + Image output shape: torch.Size([1, 5, 51200])\n"
     ]
    }
   ],
   "source": [
    "# Test with text-only input\n",
    "text = \"Hello, world!\"\n",
    "text_input = tokenizer(text, return_tensors=\"pt\")['input_ids']\n",
    "output = combined_model(text_input)\n",
    "print(\"Text-only output shape:\", output.logits.shape)\n",
    "\n",
    "# Test with text + image input\n",
    "dummy_image_embedding = torch.randn(1, 512)\n",
    "output = combined_model(text_input, dummy_image_embedding)\n",
    "print(\"Text + Image output shape:\", output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5763110-be80-480a-a149-bb0f219a8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def add_qlora_adapter(model):\n",
    "    config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"query_key_value\"],  # Adjust based on Phi model's architecture\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    return get_peft_model(model, config)\n",
    "\n",
    "# Apply QLoRA to your combined model\n",
    "qlora_model = add_qlora_adapter(combined_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e4345-ba1e-4613-9189-d2d15cba799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load LLaVA-Instruct-150K dataset\n",
    "dataset = load_dataset(\"liuhaotian/LLaVA-Instruct-150K\")\n",
    "\n",
    "# Implement a custom dataset class to handle both text and image embeddings\n",
    "class MultiModalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, clip_embeddings, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.clip_embeddings = clip_embeddings\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = f\"Human: {item['conversations'][0]['value']}\\nAssistant: {item['conversations'][1]['value']}\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "        image_embedding = self.clip_embeddings[idx]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"image_embedding\": torch.tensor(image_embedding),\n",
    "            \"labels\": inputs[\"input_ids\"].squeeze()\n",
    "        }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "train_dataset = MultiModalDataset(dataset[\"train\"], clip_embeddings, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "optimizer = torch.optim.AdamW(qlora_model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(3):  # Adjust number of epochs as needed\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        image_embeddings = batch[\"image_embedding\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = qlora_model(input_ids=input_ids, attention_mask=attention_mask, image_embedding=image_embeddings, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "qlora_model.save_pretrained(\"path_to_save_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104c2f3-5a1e-481d-ac0b-33eef8e79b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6742a242-6fbd-43b3-923d-162bdfd0bbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceba584-e50f-478c-9cd5-c09a8141672c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
