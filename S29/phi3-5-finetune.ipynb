{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\n# Load the Phi-3.5-mini-instruct model and tokenizer\nmodel_name = \"microsoft/Phi-3.5-mini-instruct\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Configure quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False\n)\n\n# Load the quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# Prepare the model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA with correct target modules\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\n        \"qkv_proj\",\n        \"o_proj\",\n        \"gate_up_proj\",\n        \"down_proj\"\n    ]\n)\n\n# Get the PEFT model\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters\nprint(\"\\nTrainable parameters:\")\nmodel.print_trainable_parameters()\n\n# Load a smaller subset of the OASST1 dataset\ndataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train[:25000]\")  # Reduced to 25000 examples\n\n# Function to format the data\ndef format_data(example):\n    instruction = example['text']\n    response = example.get('response', '')\n    prompt = f\"Instruction: {instruction}\\nResponse:\"\n    example['text'] = f\"{prompt} {response}\"\n    return example\n\n# Apply the formatting function to the dataset\nformatted_dataset = dataset.map(format_data)\n\n# Define training arguments\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results_phi_3_5_mini\",\n    num_train_epochs=2,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    save_steps=1000,\n    logging_steps=200,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n)\n\n# Define SFT trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=formatted_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=384,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n\n# Train the model\nprint(\"\\nStarting training...\")\ntrainer.train()\nprint(\"Training complete!\")\n\n# Save the model\nprint(\"\\nSaving the model...\")\ntrainer.model.save_pretrained(\"./phi_3_5_mini_finetuned\")\nprint(\"Model saved successfully!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-03T14:15:27.139113Z","iopub.execute_input":"2024-09-03T14:15:27.139538Z","iopub.status.idle":"2024-09-03T19:36:25.103187Z","shell.execute_reply.started":"2024-09-03T14:15:27.139500Z","shell.execute_reply":"2024-09-03T19:36:25.102211Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f04f87bc2f1c445493d3c272a6d033be"}},"metadata":{}},{"name":"stdout","text":"\nTrainable parameters:\ntrainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/10.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0fc2281409b456d9a15e7712294d74e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/39.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dfa50b55b1a4d978f03d4cca2f29ca8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/2.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e63a51ba03804b7385234e9350dc2fba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/84437 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"145038cb01c9499da0844611c7993c52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/4401 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"005bac5699cc4800897f183c76fdfee5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bbc116581834384be098e5e869a7414"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d42c81f14594ab2a540f130eab52cce"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:407: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240903_141631-cbqfws88</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sagar007/huggingface/runs/cbqfws88' target=\"_blank\">./results_phi_3_5_mini</a></strong> to <a href='https://wandb.ai/sagar007/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sagar007/huggingface' target=\"_blank\">https://wandb.ai/sagar007/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sagar007/huggingface/runs/cbqfws88' target=\"_blank\">https://wandb.ai/sagar007/huggingface/runs/cbqfws88</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3124' max='3124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3124/3124 5:19:21, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.794300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.681400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.663100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.675200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.634300</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.618600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.614700</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.603700</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.490100</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.505400</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.469000</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.484500</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.479100</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.479300</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.477800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Training complete!\n\nSaving the model...\nModel saved successfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model\noutput_dir = \"./phi-3.5-fine-tuned_kaggle\"\ntrainer.model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:36:26.182165Z","iopub.execute_input":"2024-09-03T19:36:26.182481Z","iopub.status.idle":"2024-09-03T19:36:26.534472Z","shell.execute_reply.started":"2024-09-03T19:36:26.182445Z","shell.execute_reply":"2024-09-03T19:36:26.533267Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Model saved to ./phi-3.5-fine-tuned_kaggle\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\n\n# Path to your saved fine-tuned model\nMODEL_PATH = \"./phi-3.5-fine-tuned_kaggle\"\n\n# Load the saved tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n# Configure quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False\n)\n\n# Load the base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/Phi-3.5-mini-instruct\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# Load the PEFT configuration\npeft_config = PeftConfig.from_pretrained(MODEL_PATH)\n\n# Load the PEFT model\nmodel = PeftModel.from_pretrained(base_model, MODEL_PATH)\n\n# Set the model to evaluation mode\nmodel.eval()\n\ndef generate_response(instruction, max_length=512):\n    # Format the input\n    prompt = f\"Instruction: {instruction}\\nResponse:\"\n    \n    # Tokenize the input\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate the response\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=1,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )\n    \n    # Decode and return the response\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response.split(\"Response:\")[1].strip()\n\n# Example usage\nif __name__ == \"__main__\":\n    while True:\n        user_input = input(\"Enter your instruction (or 'quit' to exit): \")\n        if user_input.lower() == 'quit':\n            break\n        response = generate_response(user_input)\n        print(\"Model Response:\", response)\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:38:37.281572Z","iopub.execute_input":"2024-09-03T19:38:37.282450Z","iopub.status.idle":"2024-09-03T19:40:49.737441Z","shell.execute_reply.started":"2024-09-03T19:38:37.282410Z","shell.execute_reply":"2024-09-03T19:40:49.736190Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"040fd40070cb4d29a6df34f518dfdb4f"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"Enter your instruction (or 'quit' to exit):  What are some effective ways to reduce stress?\n"},{"name":"stderr","text":"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n","output_type":"stream"},{"name":"stdout","text":"Model Response: -  Engage in regular physical activity\n  -  Practice relaxation techniques such as deep breathing, meditation, or yoga\n  -  Get enough sleep\n  -  Eat a healthy diet\n  -  Connect with others and build a support network\n  -  Prioritize self-care and take time for yourself\n  -  Set realistic goals and prioritize tasks\n  -  Learn to manage your time effectively\n  -  Seek professional help if needed\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your instruction (or 'quit' to exit):  quit\n"}]},{"cell_type":"code","source":"# First, make sure you have the latest huggingface_hub library installed\n!pip install --upgrade huggingface_hub\n\n# Import necessary libraries\nfrom huggingface_hub import HfApi\nfrom getpass import getpass\nimport os\n\n# Set your Hugging Face credentials\n# It's better to input your token this way instead of hardcoding it\nhf_token = getpass(\"Enter your Hugging Face token: \")\n\n# Set the path to your saved model\nlocal_model_path = \"./phi-3.5-fine-tuned_kaggle\"  # Replace with your actual path if different\n\n# Your specific repository ID\nrepo_id = \"sagar007/phi3.5_finetune\"\n\n# Initialize the Hugging Face API\napi = HfApi()\n\n# Create the repository if it doesn't exist\napi.create_repo(repo_id=repo_id, token=hf_token, exist_ok=True)\n\n# Upload the model files\nfor root, _, files in os.walk(local_model_path):\n    for file in files:\n        file_path = os.path.join(root, file)\n        api.upload_file(\n            path_or_fileobj=file_path,\n            path_in_repo=file_path.replace(local_model_path, \"\").lstrip(\"/\"),\n            repo_id=repo_id,\n            token=hf_token\n        )\n\nprint(f\"Model successfully pushed to {repo_id}\")\n\n# You can now use this model in your Gradio app or elsewhere by referencing:\n# MODEL_PATH = \"sagar007/phi2_finetune\"","metadata":{"execution":{"iopub.status.busy":"2024-09-03T19:41:57.670141Z","iopub.execute_input":"2024-09-03T19:41:57.670547Z","iopub.status.idle":"2024-09-03T19:42:26.988129Z","shell.execute_reply.started":"2024-09-03T19:41:57.670510Z","shell.execute_reply":"2024-09-03T19:42:26.987092Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.7.4)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your Hugging Face token:  ·····································\n"},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeb74b0ef69f47d59f16e5a19eaffb35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/50.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cd2583721c1443aa9e322c282e84bd1"}},"metadata":{}},{"name":"stdout","text":"Model successfully pushed to sagar007/phi3.5_finetune\n","output_type":"stream"}]}]}