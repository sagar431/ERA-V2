{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libraries\n!pip install -q transformers peft bitsandbytes accelerate datasets trl\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\n\n# Load the Phi-2 model and tokenizer\nmodel_name = \"microsoft/phi-2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Configure quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False\n)\n\n# Load the quantized model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# Prepare the model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Get the PEFT model\nmodel = get_peft_model(model, peft_config)\n\n# Print trainable parameters\nmodel.print_trainable_parameters()\n\n# Load a smaller subset of the OASST1 dataset\ndataset = load_dataset(\"OpenAssistant/oasst1\", split=\"train[:25000]\")  # Reduced to 25000 examples\n\n# Function to format the data\ndef format_data(example):\n    instruction = example['text']\n    response = example.get('response', '')\n    prompt = f\"Instruction: {instruction}\\nResponse:\"\n    example['text'] = f\"{prompt} {response}\"\n    return example\n\n# Apply the formatting function to the dataset\nformatted_dataset = dataset.map(format_data)\n\n# Define training arguments\ntraining_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=2,  # Reduced from 3 to 2\n    per_device_train_batch_size=8,  # Increased from 4 to 8\n    gradient_accumulation_steps=2,  # Reduced from 4 to 2\n    optim=\"paged_adamw_32bit\",\n    save_steps=1000,\n    logging_steps=200,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n)\n\n# Define SFT trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=formatted_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=384,  # Reduced from 512 to 384\n    tokenizer=tokenizer,\n    args=training_arguments,\n)\n\n# Train the model\ntrainer.train()\nprint(\"Training complete!\")\n\n# Save the model\ntrainer.model.save_pretrained(\"./phi2_finetuned_6hour\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-03T07:08:44.797486Z","iopub.execute_input":"2024-09-03T07:08:44.798190Z","iopub.status.idle":"2024-09-03T11:34:37.142896Z","shell.execute_reply.started":"2024-09-03T07:08:44.798149Z","shell.execute_reply":"2024-09-03T11:34:37.141712Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5dd496304ba4c6ba8aed7e711367fc2"}},"metadata":{}},{"name":"stdout","text":"trainable params: 9,175,040 || all params: 2,788,858,880 || trainable%: 0.3290\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a8b05d828b44d629b2b5b7f6e948580"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43f454b9eb1341e685231be60dc454f7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3124' max='3124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3124/3124 4:25:02, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>2.127400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.011800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.997700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.991200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.932400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.935000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.936600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.918600</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.839500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.864200</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.828500</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.840000</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.835500</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>1.856800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.847600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"Training complete!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model\noutput_dir = \"./phi-2-fine-tuned_kaggle\"\ntrainer.model.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T11:39:38.440447Z","iopub.execute_input":"2024-09-03T11:39:38.440816Z","iopub.status.idle":"2024-09-03T11:39:38.862387Z","shell.execute_reply.started":"2024-09-03T11:39:38.440775Z","shell.execute_reply":"2024-09-03T11:39:38.861342Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model saved to ./phi-2-fine-tuned_kaggle\n","output_type":"stream"}]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2024-09-03T11:40:49.149813Z","iopub.execute_input":"2024-09-03T11:40:49.150526Z","iopub.status.idle":"2024-09-03T11:40:49.157436Z","shell.execute_reply.started":"2024-09-03T11:40:49.150486Z","shell.execute_reply":"2024-09-03T11:40:49.156530Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\n\n# Path to your saved fine-tuned model\nMODEL_PATH = \"./phi-2-fine-tuned_kaggle\"\n\n# Load the saved tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n\n# Configure quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False\n)\n\n# Load the base model\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/phi-2\",\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n\n# Load the PEFT configuration\npeft_config = PeftConfig.from_pretrained(MODEL_PATH)\n\n# Load the PEFT model\nmodel = PeftModel.from_pretrained(base_model, MODEL_PATH)\n\n# Set the model to evaluation mode\nmodel.eval()\n\ndef generate_response(instruction, max_length=512):\n    # Format the input\n    prompt = f\"Instruction: {instruction}\\nResponse:\"\n    \n    # Tokenize the input\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    # Generate the response\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=1,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )\n    \n    # Decode and return the response\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response.split(\"Response:\")[1].strip()\n\n# Example usage\nif __name__ == \"__main__\":\n    while True:\n        user_input = input(\"Enter your instruction (or 'quit' to exit): \")\n        if user_input.lower() == 'quit':\n            break\n        response = generate_response(user_input)\n        print(\"Model Response:\", response)\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T11:46:18.055572Z","iopub.execute_input":"2024-09-03T11:46:18.055952Z","iopub.status.idle":"2024-09-03T11:47:25.522775Z","shell.execute_reply.started":"2024-09-03T11:46:18.055916Z","shell.execute_reply":"2024-09-03T11:47:25.521357Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6266fba4bb154ebab33224e3244e361e"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"Enter your instruction (or 'quit' to exit):  Explain the concept of machine learning.\n"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Model Response: The concept of machine learning is a branch of artificial intelligence that involves the development of algorithms and statistical models that allow computers to learn from data, without being explicitly programmed.\n\nMachine learning algorithms can automatically improve their performance on a specific task by analyzing and interpreting data. They can identify patterns and relationships within the data, and use this information to make predictions or decisions.\n\nMachine learning is used in a wide range of applications, including natural language processing, image and speech recognition, fraud detection, and recommendation systems.\n\nThe process of machine learning typically involves the following steps:\n\n1. Data collection: The first step is to collect a large amount of data related to the problem to be solved.\n\n2. Data preprocessing: The data is then cleaned, normalized, and transformed into a format that can be used by the machine learning algorithm.\n\n3. Model training: The machine learning algorithm is trained on the preprocessed data to identify patterns and relationships.\n\n4. Model evaluation: The trained model is evaluated using a separate set of data to determine its accuracy and performance.\n\n5. Model deployment: Once the model has been trained and evaluated, it can be deployed in a real-world application.\n\nMachine learning is a rapidly evolving field, and new techniques and algorithms are constantly being developed to improve the accuracy and efficiency of machine learning models.\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m         user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter your instruction (or \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m to exit): \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}]},{"cell_type":"code","source":"# First, make sure you have the latest huggingface_hub library installed\n!pip install --upgrade huggingface_hub\n\n# Import necessary libraries\nfrom huggingface_hub import HfApi\nfrom getpass import getpass\nimport os\n\n# Set your Hugging Face credentials\n# It's better to input your token this way instead of hardcoding it\nhf_token = getpass(\"Enter your Hugging Face token: \")\n\n# Set the path to your saved model\nlocal_model_path = \"./phi-2-fine-tuned_kaggle\"  # Replace with your actual path if different\n\n# Your specific repository ID\nrepo_id = \"sagar007/phi2_25k\"\n\n# Initialize the Hugging Face API\napi = HfApi()\n\n# Create the repository if it doesn't exist\napi.create_repo(repo_id=repo_id, token=hf_token, exist_ok=True)\n\n# Upload the model files\nfor root, _, files in os.walk(local_model_path):\n    for file in files:\n        file_path = os.path.join(root, file)\n        api.upload_file(\n            path_or_fileobj=file_path,\n            path_in_repo=file_path.replace(local_model_path, \"\").lstrip(\"/\"),\n            repo_id=repo_id,\n            token=hf_token\n        )\n\nprint(f\"Model successfully pushed to {repo_id}\")\n\n# You can now use this model in your Gradio app or elsewhere by referencing:\n# MODEL_PATH = \"sagar007/phi2_finetune\"","metadata":{"execution":{"iopub.status.busy":"2024-09-03T11:47:31.550025Z","iopub.execute_input":"2024-09-03T11:47:31.550414Z","iopub.status.idle":"2024-09-03T11:48:25.605094Z","shell.execute_reply.started":"2024-09-03T11:47:31.550374Z","shell.execute_reply":"2024-09-03T11:48:25.604107Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.24.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.7.4)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your Hugging Face token:  ·····································\n"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/36.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17dd500cca044a6c98680e3537e5420f"}},"metadata":{}},{"name":"stdout","text":"Model successfully pushed to sagar007/phi2_25k\n","output_type":"stream"}]}]}